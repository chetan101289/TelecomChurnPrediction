{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem Overview\n",
    "In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition. \n",
    "\n",
    "For many incumbent operators, retaining high profitable customers is the number one business goal. \n",
    "\n",
    "To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\n",
    "\n",
    "#### Definitions of Churn\n",
    "\n",
    "<B>Usage-based churn</b>: Customers who have not done any usage, either incoming or outgoing - in terms of calls, internet etc. over a period of time. In this project, we will use the <b>usage-based definition</b> to define churn.\n",
    "\n",
    "Analysis of only high values customer to be done in this case study. Defincation of high values customer is as follow\n",
    "\n",
    "2. Filter high-value customers\n",
    "Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).\n",
    "\n",
    "Expectation is to build two model \n",
    "1. It will be used to predict whether a high-value customer will churn or not, in near future (i.e. churn phase).\n",
    "\n",
    "2. It will be used to identify important variables that are strong predictors of churn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    " # trying to impute the continuous null features values using KNNImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "# splitter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# scaler\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# pca\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "# GridSearch CV for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# for showing tree\n",
    "from IPython.display import Image\n",
    "from six import StringIO # use six instead of sklearn.externals.six\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus, graphviz\n",
    "\n",
    "# display upto 3 decimals\n",
    "pd.options.display.float_format = \"{:,.3f}\".format\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option ('display.max_columns', None) \n",
    "pd.set_option ('display.max_rows', 999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will be performing following steps to address the problem in hand \n",
    "1. Data Load and Cleaning\n",
    "2. EDA\n",
    "3. Data Visualisation\n",
    "4. Model Building \n",
    "5. Model Validation\n",
    "6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Load and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv\n",
    "churn_df = pd.read_csv(\"telecom_churn_data.csv\")\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature type summary\n",
    "churn_df.info(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the missing values with %\n",
    "missing = (churn_df.isna().sum()/churn_df.shape[0])*100\n",
    "missing.reset_index()\n",
    "#Create Datafame\n",
    "df_missing = pd.DataFrame(missing)\n",
    "\n",
    "#reset index\n",
    "df_missing = df_missing.reset_index()\n",
    "#Set Column names\n",
    "df_missing.columns = ['Name','MissingValuesPer']\n",
    "df_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_missing[df_missing.MissingValuesPer >0]['Name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column name list by types of columns\n",
    "drop_columns = ['mobile_number', 'circle_id', 'last_date_of_month_6',\n",
    "             'last_date_of_month_7',\n",
    "             'last_date_of_month_8',\n",
    "             'last_date_of_month_9',\n",
    "             'date_of_last_rech_6',\n",
    "             'date_of_last_rech_7',\n",
    "             'date_of_last_rech_8',\n",
    "             'date_of_last_rech_9',\n",
    "             'date_of_last_rech_data_6',\n",
    "             'date_of_last_rech_data_7',\n",
    "             'date_of_last_rech_data_8',\n",
    "             'date_of_last_rech_data_9']\n",
    "\n",
    "# drop these coulmns as it would not be significant in the churn analysis as we will be performing usage based anaylysis.\n",
    "churn_df = churn_df.drop(drop_columns, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List down the category columns which would be imputed with -1 for missing values\n",
    "cat_cols =  ['night_pck_user_6',\n",
    "             'night_pck_user_7',\n",
    "             'night_pck_user_8',\n",
    "             'night_pck_user_9',\n",
    "             'fb_user_6',\n",
    "             'fb_user_7',\n",
    "             'fb_user_8',\n",
    "             'fb_user_9'\n",
    "            ]\n",
    "\n",
    "# Imput -1 for missing continuous variable values\n",
    "churn_df[cat_cols] = churn_df[cat_cols].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df[cat_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List down all the continuos variables \n",
    "cont_var = [column for column in churn_df.columns if column not in drop_columns +  cat_cols]\n",
    "\n",
    "# print the number of columns in each list\n",
    "print(\"#Dropped cols: %d\\n#Continuous Variables:%d\\n#Category cols:%d\" % (len(drop_columns), len(cont_var), len(cat_cols)))\n",
    "\n",
    "# check if we have missed any column or not\n",
    "print(len(cont_var) + len(cat_cols) == churn_df.shape[1])\n",
    "churn_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ll values in these columns are zero or nan. Dropig these columns as it has no variance \n",
    "churn_df[['loc_og_t2o_mou','std_og_t2o_mou','loc_ic_t2o_mou','std_og_t2c_mou_6',\n",
    "                     'std_og_t2c_mou_7','std_og_t2c_mou_8','std_ic_t2o_mou_6','std_ic_t2o_mou_7',\n",
    "                   'std_ic_t2o_mou_8']].value_counts()\n",
    "\n",
    "churn_df = churn_df.drop(['loc_og_t2o_mou','std_og_t2o_mou','loc_ic_t2o_mou','std_og_t2c_mou_6',\n",
    "                     'std_og_t2c_mou_7','std_og_t2c_mou_8','std_ic_t2o_mou_6','std_ic_t2o_mou_7',\n",
    "                   'std_ic_t2o_mou_8'], axis=1)\n",
    "\n",
    "churn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some recharge columns have minimum value of 1 while some don't\n",
    "recharge_cols = ['total_rech_data_6', 'total_rech_data_7', 'total_rech_data_8', 'total_rech_data_9',\n",
    "                 'count_rech_2g_6', 'count_rech_2g_7', 'count_rech_2g_8', 'count_rech_2g_9',\n",
    "                 'count_rech_3g_6', 'count_rech_3g_7', 'count_rech_3g_8', 'count_rech_3g_9',\n",
    "                 'max_rech_data_6', 'max_rech_data_7', 'max_rech_data_8', 'max_rech_data_9',\n",
    "                 'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8', 'av_rech_amt_data_9',\n",
    "                 ]\n",
    "\n",
    "churn_df[recharge_cols].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute these columns with zero\n",
    "churn_df[recharge_cols] = churn_df[recharge_cols].fillna(0)\n",
    "churn_df[recharge_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(churn_df.isna().sum()/churn_df.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the missing values with %\n",
    "missing = (churn_df.isna().sum()/churn_df.shape[0])*100\n",
    "missing.reset_index()\n",
    "#Create Datafame\n",
    "df_missing = pd.DataFrame(missing)\n",
    "\n",
    "#reset index\n",
    "df_missing = df_missing.reset_index()\n",
    "#Set Column names\n",
    "df_missing.columns = ['Name','MissingValuesPer']\n",
    "df_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with more than 70% empty values\n",
    "churn_df = churn_df.drop( df_missing[df_missing.MissingValuesPer>70]['Name'].values, axis=1)\n",
    "churn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imput 0 in missing values to all features related to calls.\n",
    "churn_df['onnet_mou_6'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df_missing[(df_missing.MissingValuesPer >0) & (df_missing.MissingValuesPer <70)]['Name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values as a matrix \n",
    "msno.matrix(churn_df[missing_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all the columns with missing values for month 6,7 and 8\n",
    "col_6_names = df_missing[(df_missing.MissingValuesPer>0) & (df_missing.MissingValuesPer <= 70) & df_missing.Name.str.contains('6$')]['Name'].values\n",
    "col_7_names = df_missing[(df_missing.MissingValuesPer>0) & (df_missing.MissingValuesPer <= 70) & df_missing.Name.str.contains('7$')]['Name'].values\n",
    "col_8_names = df_missing[(df_missing.MissingValuesPer>0) & (df_missing.MissingValuesPer <= 70) & df_missing.Name.str.contains('8$')]['Name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomeFillNa(row, col, total_rec_amt):\n",
    "    print(row)\n",
    "    if((row[col] == np.nan) & (row[total_rec_amt] == 0)):\n",
    "        return 0\n",
    "    else:\n",
    "        return row[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute values to 0 form month in which thre was no recharge i.e. total_recharge_amount =0 \n",
    "churn_df[(churn_df.onnet_mou_6 == np.nan) & (churn_df.total_rech_amt_6 == 0)][col_6_names]\n",
    "\n",
    "#impute all these values to 0, as total recharge amount is 0 and values are missing for all such features, \n",
    "# we assume customer was inactive during this time\n",
    "# imputing all such columns to 0\n",
    "\n",
    "churn_df[(churn_df.onnet_mou_6.isna()) ][col_6_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df[(churn_df.onnet_mou_6.isna()) & (churn_df.total_rech_amt_6 == 0)][col_6_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df[col_6_names].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pending imputing missing values\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking mod to impute the misssing values in the columns\n",
    "print(churn_df.onnet_mou_6.head())\n",
    "churn_df[['onnet_mou_6']].fillna(churn_df.onnet_mou_6.mode()[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA\n",
    "#### Derive Churn\n",
    "9th Month is our Churn Phase. \n",
    "- Usage-based churn\n",
    "Calculate total incoming and outgoing minutes of usage\n",
    "Calculate 2g and 3g data consumption\n",
    "Create churn variable: those who have not used either calls or internet in the month of September are customers who have churned\n",
    "Check Churn percentage.\n",
    "Delete columns that belong to the churn month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check nulls\n",
    "churn_df[['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create total calls in 9th month\n",
    "churn_df['total_calls_mou_9'] = churn_df.total_ic_mou_9 + churn_df.total_og_mou_9\n",
    "\n",
    "#create total data usage derived column\n",
    "churn_df['total_data_mb_9'] = churn_df.vol_2g_mb_9 + churn_df.vol_3g_mb_9\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df[['total_calls_mou_9','total_data_mb_9']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create churn column\n",
    "# 1- churned 0- not Churned\n",
    "churn_df['churned'] = churn_df.apply(lambda row: 1 if ((row['total_calls_mou_9'] + row['total_data_mb_9']) == 0) else 0, axis=1)\n",
    "churn_df.churned =  churn_df.churned.astype('category')\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check churn percentage on total data\n",
    "round(churn_df.churned.value_counts()/churn_df.shape[0] *100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define high values customer and filter the data set.\n",
    "\n",
    "We need to predict churn only for the high-value customers. Define high-value customers as follows: Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).\n",
    "\n",
    "- calculate total recharge amount\n",
    "\n",
    "    call recharge amount( total_rech_amt ) + data recharge amount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check nulls\n",
    "print(churn_df[['total_rech_amt_6','total_rech_amt_7','av_rech_amt_data_6','total_rech_data_6','av_rech_amt_data_7','total_rech_data_7']].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived Column Average Total Recharge Amount for 6th and 7th (first two) months\n",
    "churn_df['total_avg_rech_amt_6n7'] = round((churn_df.total_rech_amt_6 + churn_df.total_rech_amt_7 + \n",
    "(churn_df.av_rech_amt_data_6 * churn_df.total_rech_data_6) + (churn_df.av_rech_amt_data_7 * churn_df.total_rech_data_7))/2, 2)\n",
    "\n",
    "churn_df['total_avg_rech_amt_6n7'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70th quantile of average of first two month of recharge. Ignored the records where re charge amount is 0\n",
    "Q70 = np.quantile(churn_df.total_avg_rech_amt_6n7, 0.70)\n",
    "print(Q70)\n",
    "churn_df[churn_df.total_avg_rech_amt_6n7 >= Q70].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use filter data set for further processing\n",
    "telecom_df = churn_df[churn_df.total_avg_rech_amt_6n7 >= Q70]\n",
    "telecom_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check churn percentage on total data\n",
    "round(telecom_df.churned.value_counts()/telecom_df.shape[0] *100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pie chart depicting the churn customer as pert of full portfolio.\n",
    "(telecom_df['churned'].value_counts(1)*100).plot(kind='pie')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete columns from 9th months\n",
    "col_9_names = churn_df.filter(regex='9$', axis=1).columns\n",
    "telecom_df = telecom_df.drop(col_9_names, axis=1)\n",
    "telecom_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop derived column created for filtering high value customer.\n",
    "telecom_df = telecom_df.drop('total_avg_rech_amt_6n7', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values treatment using KNNImputer\n",
    "\n",
    "Using KNN Imputer to impute missing data in the filtered dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values\n",
    "ms_df = telecom_df.isnull().sum()/len(telecom_df.index)*100\n",
    "ms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe\n",
    "ms_df = pd.DataFrame(ms_df, columns=['MissingVal'])\n",
    "ms_df = ms_df.reset_index()\n",
    "ms_df = ms_df.rename(columns={'index':'Column'})\n",
    "\n",
    "# selecting only missing values\n",
    "missing_values_columns = ms_df[ms_df.MissingVal > 0]['Column'].tolist()\n",
    "\n",
    "# selecting only continuous variables for imputing\n",
    "missing_values_columns = telecom_df[missing_values_columns].select_dtypes(include='float64').columns.tolist()\n",
    "\n",
    "# checking the number of variables for imputing\n",
    "len(missing_values_columns)\n",
    "#len(churn_df.columns[churn_df.columns.str.contains('9$')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define imputer\n",
    "imputer = KNNImputer(n_neighbors= 5)\n",
    "\n",
    "# fit on the dataset\n",
    "for col in missing_values_columns:\n",
    "    print(col)\n",
    "    telecom_df[col] = imputer.fit_transform(telecom_df[col].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the difference between 8th and previous months.\n",
    "\n",
    "The ‘good’ phase: In this phase, the customer is happy with the service and behaves as usual.\n",
    "\n",
    "The ‘action’ phase: The customer experience starts to sore in this phase, for e.g. he/she gets a compelling offer from a  competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the ‘good’ months. Also, it is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor’s offer/improving the service quality etc.)\n",
    "\n",
    "In our case 6 and 7th are the good phase where as 8th is action phase. Hence its important to compare the service usage bewteen the average usage during 6th anf 7th with that of 8th month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the variance between 6 and 7th months with that of 8th for continuous variables.\n",
    "telecom_df['arpu_var'] = telecom_df.arpu_8 - ((telecom_df.arpu_6 + telecom_df.arpu_7)/2)\n",
    "\n",
    "telecom_df['onnet_mou_var'] = telecom_df.onnet_mou_8 - ((telecom_df.onnet_mou_6 + telecom_df.onnet_mou_7)/2)\n",
    "\n",
    "telecom_df['offnet_mou_var'] = telecom_df.offnet_mou_8 - ((telecom_df.offnet_mou_6 + telecom_df.offnet_mou_7)/2)\n",
    "\n",
    "telecom_df['roam_ic_mou_var'] = telecom_df.roam_ic_mou_8 - ((telecom_df.roam_ic_mou_6 + telecom_df.roam_ic_mou_7)/2)\n",
    "\n",
    "telecom_df['roam_og_mou_var'] = telecom_df.roam_og_mou_8 - ((telecom_df.roam_og_mou_6 + telecom_df.roam_og_mou_7)/2)\n",
    "\n",
    "telecom_df['loc_og_mou_var'] = telecom_df.loc_og_mou_8 - ((telecom_df.loc_og_mou_6 + telecom_df.loc_og_mou_7)/2)\n",
    "\n",
    "telecom_df['std_og_mou_var'] = telecom_df.std_og_mou_8 - ((telecom_df.std_og_mou_6 + telecom_df.std_og_mou_7)/2)\n",
    "\n",
    "telecom_df['isd_og_mou_var'] = telecom_df.isd_og_mou_8 - ((telecom_df.isd_og_mou_6 + telecom_df.isd_og_mou_7)/2)\n",
    "\n",
    "telecom_df['spl_og_mou_var'] = telecom_df.spl_og_mou_8 - ((telecom_df.spl_og_mou_6 + telecom_df.spl_og_mou_7)/2)\n",
    "\n",
    "telecom_df['total_og_mou_var'] = telecom_df.total_og_mou_8 - ((telecom_df.total_og_mou_6 + telecom_df.total_og_mou_7)/2)\n",
    "\n",
    "telecom_df['loc_ic_mou_var'] = telecom_df.loc_ic_mou_8 - ((telecom_df.loc_ic_mou_6 + telecom_df.loc_ic_mou_7)/2)\n",
    "\n",
    "telecom_df['std_ic_mou_var'] = telecom_df.std_ic_mou_8 - ((telecom_df.std_ic_mou_6 + telecom_df.std_ic_mou_7)/2)\n",
    "\n",
    "telecom_df['isd_ic_mou_var'] = telecom_df.isd_ic_mou_8 - ((telecom_df.isd_ic_mou_6 + telecom_df.isd_ic_mou_7)/2)\n",
    "\n",
    "telecom_df['spl_ic_mou_var'] = telecom_df.spl_ic_mou_8 - ((telecom_df.spl_ic_mou_6 + telecom_df.spl_ic_mou_7)/2)\n",
    "\n",
    "telecom_df['total_ic_mou_var'] = telecom_df.total_ic_mou_8 - ((telecom_df.total_ic_mou_6 + telecom_df.total_ic_mou_7)/2)\n",
    "\n",
    "telecom_df['total_rech_num_var'] = telecom_df.total_rech_num_8 - ((telecom_df.total_rech_num_6 + telecom_df.total_rech_num_7)/2)\n",
    "\n",
    "telecom_df['total_rech_amt_var'] = telecom_df.total_rech_amt_8 - ((telecom_df.total_rech_amt_6 + telecom_df.total_rech_amt_7)/2)\n",
    "\n",
    "telecom_df['max_rech_amt_var'] = telecom_df.max_rech_amt_8 - ((telecom_df.max_rech_amt_6 + telecom_df.max_rech_amt_7)/2)\n",
    "\n",
    "telecom_df['total_rech_data_var'] = telecom_df.total_rech_data_8 - ((telecom_df.total_rech_data_6 + telecom_df.total_rech_data_7)/2)\n",
    "\n",
    "telecom_df['max_rech_data_var'] = telecom_df.max_rech_data_8 - ((telecom_df.max_rech_data_6 + telecom_df.max_rech_data_7)/2)\n",
    "\n",
    "telecom_df['av_rech_amt_data_var'] = telecom_df.av_rech_amt_data_8 - ((telecom_df.av_rech_amt_data_6 + telecom_df.av_rech_amt_data_7)/2)\n",
    "\n",
    "telecom_df['vol_2g_mb_var'] = telecom_df.vol_2g_mb_8 - ((telecom_df.vol_2g_mb_6 + telecom_df.vol_2g_mb_7)/2)\n",
    "\n",
    "telecom_df['vol_3g_mb_var'] = telecom_df.vol_3g_mb_8 - ((telecom_df.vol_3g_mb_6 + telecom_df.vol_3g_mb_7)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Visualization\n",
    "\n",
    "#### Univariate Data Analysis\n",
    "\n",
    "In this section we will analyse and understand the various variables in the dateset. Since there are more than 100 variables/features in the datase will plot few which we think are important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common function to plot the data\n",
    "def UnivariatePlot(feature, is_categorical, viewStats=True):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    if(viewStats):        \n",
    "        print(feature.describe())\n",
    "    if(is_categorical):  \n",
    "        plt.title('Count Plot')\n",
    "        sns.countplot(feature)\n",
    "    else:\n",
    "        \n",
    "        plt.title('Distribution Plot')\n",
    "        sns.distplot(feature)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the continuous variables \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Distribution Plot')       \n",
    "plt.subplot(2,2,1)\n",
    "sns.distplot(telecom_df.total_rech_amt_6)\n",
    "plt.subplot(2,2,2)\n",
    "sns.distplot(telecom_df.total_rech_amt_7)\n",
    "plt.subplot(2,2,3)\n",
    "sns.distplot(telecom_df.total_rech_amt_8)\n",
    "\n",
    "# data shows the recharge amount it concentrated between 0 and 2500 for each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the number of recharges for 6,7,8 months variables \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Distribution Plot')       \n",
    "plt.subplot(2,2,1)\n",
    "sns.distplot(telecom_df.total_rech_num_6)\n",
    "plt.subplot(2,2,2)\n",
    "sns.distplot(telecom_df.total_rech_num_7)\n",
    "plt.subplot(2,2,3)\n",
    "sns.distplot(telecom_df.total_rech_num_8)\n",
    "\n",
    "# Again not very clear trend but number of reacharges seems ot have fallen in the month 7 and 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the total recharge data for 6,7,8 months variables \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Distribution Plot')       \n",
    "plt.subplot(2,2,1)\n",
    "sns.distplot(telecom_df.total_rech_data_6)\n",
    "plt.subplot(2,2,2)\n",
    "sns.distplot(telecom_df.total_rech_data_7)\n",
    "plt.subplot(2,2,3)\n",
    "sns.distplot(telecom_df.total_rech_data_8)\n",
    "\n",
    "# Not much variance between three months data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the average data recharge amount for 6,7,8 months variables \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Distribution Plot')       \n",
    "plt.subplot(2,2,1)\n",
    "sns.distplot(telecom_df.av_rech_amt_data_6)\n",
    "plt.subplot(2,2,2)\n",
    "sns.distplot(telecom_df.av_rech_amt_data_7)\n",
    "plt.subplot(2,2,3)\n",
    "sns.distplot(telecom_df.av_rech_amt_data_8)\n",
    "plt.show()\n",
    "# Not much variance between three months data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the average data recharge amount for 6,7,8 months variables \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Distribution Plot')       \n",
    "plt.subplot(2,2,1)\n",
    "sns.distplot(telecom_df.arpu_6)\n",
    "plt.subplot(2,2,2)\n",
    "sns.distplot(telecom_df.arpu_7)\n",
    "plt.subplot(2,2,3)\n",
    "sns.distplot(telecom_df.arpu_8)\n",
    "plt.show()\n",
    "# Not much variance between three months data. There seems to be sligh drop in october."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UnivariatePlot(telecom_df.fb_user_6, True, True)\n",
    "#Plotting the average data recharge amount for 6,7,8 months variables \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Count Plot')       \n",
    "plt.subplot(2,2,1)\n",
    "sns.countplot(telecom_df.fb_user_6)\n",
    "plt.subplot(2,2,2)\n",
    "sns.countplot(telecom_df.fb_user_7)\n",
    "plt.subplot(2,2,3)\n",
    "sns.countplot(telecom_df.fb_user_8)\n",
    "plt.show()\n",
    "#There seems to be slight increase in number of user not opting for this feature in october"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UnivariatePlot(telecom_df.fb_user_6, True, True)\n",
    "#Plotting the average data recharge amount for 6,7,8 months variables \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Count Plot')       \n",
    "plt.subplot(2,2,1)\n",
    "sns.countplot(telecom_df.night_pck_user_6)\n",
    "plt.subplot(2,2,2)\n",
    "sns.countplot(telecom_df.night_pck_user_7)\n",
    "plt.subplot(2,2,3)\n",
    "sns.countplot(telecom_df.night_pck_user_8)\n",
    "plt.show()\n",
    "#There seems to be slight increase in number of user not opting for this feature in october"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#UnivariatePlot(telecom_df.fb_user_6, True, True)\n",
    "#Plotting the last day recharge for 6,7,8 months variables \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Count Plot')       \n",
    "plt.subplot(2,2,1)\n",
    "sns.distplot(telecom_df.last_day_rch_amt_6)\n",
    "plt.subplot(2,2,2)\n",
    "sns.distplot(telecom_df.last_day_rch_amt_7)\n",
    "plt.subplot(2,2,3)\n",
    "sns.distplot(telecom_df.last_day_rch_amt_7)\n",
    "plt.show()\n",
    "#There seems to be little to no variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  avg recharge mant for data\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Count Plot')       \n",
    "plt.subplot(2,2,1)\n",
    "sns.distplot(telecom_df.av_rech_amt_data_6)\n",
    "plt.subplot(2,2,2)\n",
    "sns.distplot(telecom_df.av_rech_amt_data_7)\n",
    "plt.subplot(2,2,3)\n",
    "sns.distplot(telecom_df.av_rech_amt_data_8)\n",
    "plt.show()\n",
    "#There seems to be fall in average recharge amount in octber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Visualization of Data\n",
    "\n",
    "In this section we will plot the bivariate plots for the both continuos and categorical variables against target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutomer perios with network against churn data. \n",
    "plt.figure(figsize=(12,6))   \n",
    "sns.boxplot(telecom_df.aon,telecom_df.churned )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutomer perios with network against churn data. \n",
    "plt.figure(figsize=(12,6))   \n",
    "sns.boxplot(data=telecom_df, x='total_rech_data_6',y=telecom_df.churned )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will plot the continuous variables against in pairplot\n",
    "sns.pairplot(data=telecom_df[['aon','total_rech_amt_6','total_rech_amt_7','total_rech_amt_8','total_rech_num_6','total_rech_num_7','total_rech_num_8','churned']],hue='churned' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will plot the continuous variables against in pairplot\n",
    "sns.pairplot(data=telecom_df[['arpu_6','arpu_7','arpu_8','churned']],hue='churned' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will plot the continuous variables against in pairplot\n",
    "sns.pairplot(data=telecom_df[['arpu_var','total_rech_amt_var','av_rech_amt_data_var','churned']],hue='churned' )\n",
    "plt.show()\n",
    "# we can clearly see when variance is 0 or lower chances of customer being churn are higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will plot the continuous variables against in pairplot\n",
    "sns.pairplot(data=telecom_df[['total_og_mou_var','total_ic_mou_var','vol_2g_mb_var','vol_3g_mb_var','churned']],hue='churned' )\n",
    "plt.show()\n",
    "# its clearly visible from the plot that as calls and data usage fall in 8th months resulted in the customer being churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas cross tab for the categorical values\n",
    "pd.crosstab(telecom_df.churned, [telecom_df.night_pck_user_6,telecom_df.night_pck_user_7,telecom_df.night_pck_user_8], normalize='columns')*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas cross tab for the categorical values\n",
    "pd.crosstab(telecom_df.churned, [telecom_df.fb_user_6,telecom_df.fb_user_7,telecom_df.fb_user_8], normalize='columns')*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas cross tab for the categorical values\n",
    "pd.crosstab(telecom_df.churned, [telecom_df.monthly_2g_6, telecom_df.monthly_2g_7, telecom_df.monthly_2g_8], normalize='columns')*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list for each models param and performance.\n",
    "ModelName = []\n",
    "Accuracy = []\n",
    "Sensitivity = []\n",
    "Specificity = []\n",
    "ROC = []\n",
    "AUC=[]\n",
    "Param = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Modeling \n",
    "\n",
    "In this section we will create model for churn prediction and to explain the important variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Outlier treatment of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the data, we can see that there are some significant outliers and there are so many values of outliers, which means that they are not actually outliers but the high value customers with more usage. And removing those values as outliers are not suitable as it can impact the model accuracy. Also, we are losing a lot of data if we remove these values. Let's continue with all the values and see the model performance first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking the outliers by describing the data and looking at diff percentile values\n",
    "telecom_df.describe(percentiles=[0.1,0.25,0.5,0.75,0.80,0.85,0.9,0.95,0.99,0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting boxplots\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.boxplot(data = telecom_df[['arpu_6', 'arpu_7', 'arpu_8']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# plotting boxplots\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.boxplot(data = telecom_df[['onnet_mou_6', 'onnet_mou_7', 'onnet_mou_8']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns for outlier treatment.\n",
    "columns= ['arpu_6', 'arpu_7',\t'arpu_8',\t'onnet_mou_6',\t'onnet_mou_7',\t'onnet_mou_8',\t'offnet_mou_6',\t'offnet_mou_7',\t'offnet_mou_8',\t'roam_ic_mou_6',\t'roam_ic_mou_7',\t'roam_ic_mou_8',\t'roam_og_mou_6',\t'roam_og_mou_7',\t'roam_og_mou_8',\t'loc_og_t2t_mou_6',\t'loc_og_t2t_mou_7',\t'loc_og_t2t_mou_8',\t'loc_og_t2m_mou_6',\t'loc_og_t2m_mou_7',\t'loc_og_t2m_mou_8',\t'loc_og_t2f_mou_6',\t'loc_og_t2f_mou_7',\t'loc_og_t2f_mou_8',\t'loc_og_t2c_mou_6',\t'loc_og_t2c_mou_7',\t'loc_og_t2c_mou_8',\t'loc_og_mou_6',\t'loc_og_mou_7',\t'loc_og_mou_8',\t'std_og_t2t_mou_6',\t'std_og_t2t_mou_7',\t'std_og_t2t_mou_8',\t'std_og_t2m_mou_6',\t'std_og_t2m_mou_7',\t'std_og_t2m_mou_8',\t'std_og_t2f_mou_6',\t'std_og_t2f_mou_7',\t'std_og_t2f_mou_8',\t'std_og_mou_6',\t'std_og_mou_7',\t'std_og_mou_8',\t'isd_og_mou_6',\t'isd_og_mou_7',\t'isd_og_mou_8',\t'spl_og_mou_6',\t'spl_og_mou_7',\t'spl_og_mou_8',\t'og_others_6',\t'og_others_7',\t'og_others_8',\t'total_og_mou_6',\t'total_og_mou_7',\t'total_og_mou_8',\t'loc_ic_t2t_mou_6',\t'loc_ic_t2t_mou_7',\t'loc_ic_t2t_mou_8',\t'loc_ic_t2m_mou_6',\t'loc_ic_t2m_mou_7',\t'loc_ic_t2m_mou_8',\t'loc_ic_t2f_mou_6',\t'loc_ic_t2f_mou_7',\t'loc_ic_t2f_mou_8',\t'loc_ic_mou_6',\t'loc_ic_mou_7',\t'loc_ic_mou_8',\t'std_ic_t2t_mou_6',\t'std_ic_t2t_mou_7',\t'std_ic_t2t_mou_8',\t'std_ic_t2m_mou_6',\t'std_ic_t2m_mou_7',\t'std_ic_t2m_mou_8',\t'std_ic_t2f_mou_6',\t'std_ic_t2f_mou_7',\t'std_ic_t2f_mou_8',\t'std_ic_mou_6',\t'std_ic_mou_7',\t'std_ic_mou_8',\t'total_ic_mou_6',\t'total_ic_mou_7',\t'total_ic_mou_8',\t'spl_ic_mou_6',\t'spl_ic_mou_7',\t'spl_ic_mou_8',\t'isd_ic_mou_6',\t'isd_ic_mou_7',\t'isd_ic_mou_8',\t'ic_others_6',\t'ic_others_7',\t'ic_others_8',\t'total_rech_num_6',\t'total_rech_num_7',\t'total_rech_num_8',\t'total_rech_amt_6',\t'total_rech_amt_7',\t'total_rech_amt_8',\t'max_rech_amt_6',\t'max_rech_amt_7',\t'max_rech_amt_8',\t'last_day_rch_amt_6',\t'last_day_rch_amt_7',\t'last_day_rch_amt_8',\t'total_rech_data_6',\t'total_rech_data_7',\t'total_rech_data_8',\t'max_rech_data_6',\t'max_rech_data_7',\t'max_rech_data_8',\t'count_rech_2g_6',\t'count_rech_2g_7',\t'count_rech_2g_8',\t'count_rech_3g_6',\t'count_rech_3g_7',\t'count_rech_3g_8',\t'av_rech_amt_data_6',\t'av_rech_amt_data_7',\t'av_rech_amt_data_8',\t'vol_2g_mb_6',\t'vol_2g_mb_7',\t'vol_2g_mb_8',\t'vol_3g_mb_6',\t'vol_3g_mb_7',\t'vol_3g_mb_8',\t'aon',\t'aug_vbc_3g',\t'jul_vbc_3g',\t'jun_vbc_3g',\t'sep_vbc_3g',\t'arpu_var',\t'onnet_mou_var',\t'offnet_mou_var',\t'roam_ic_mou_var',\t'roam_og_mou_var',\t'loc_og_mou_var',\t'std_og_mou_var',\t'isd_og_mou_var',\t'spl_og_mou_var',\t'total_og_mou_var',\t'loc_ic_mou_var',\t'std_ic_mou_var',\t'isd_ic_mou_var',\t'spl_ic_mou_var',\t'total_ic_mou_var',\t'total_rech_num_var',\t'total_rech_amt_var',\t'max_rech_amt_var',\t'total_rech_data_var',\t'max_rech_data_var',\t'av_rech_amt_data_var',\t'vol_2g_mb_var',\t'vol_3g_mb_var'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in columns:\n",
    "    Q98= telecom_df[col].quantile(0.98)\n",
    "    print(col +' :', Q98,len(telecom_df[telecom_df[col] >Q98]))\n",
    "    telecom_df[col] = telecom_df[col].apply(lambda x: x if x <Q98 else Q98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and testing dataset.\n",
    "\n",
    "We will use sklearns train_test_split module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratify will ensure we will have same proportion of churned data in the split\n",
    "X = telecom_df.drop(\"churned\", axis = 1)\n",
    "y = telecom_df.churned\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 45, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The class weight is used to handle class imbalance - it adjusts the cost function\n",
    "forest = RandomForestClassifier(class_weight={0:0.1, 1: 0.9}, n_jobs = -1)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {\"criterion\": ['gini', 'entropy'], \"max_features\": ['auto', 0.4]}\n",
    "\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "rf_model = GridSearchCV(estimator=forest, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model \n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model with best parameters \n",
    "print(\"Best AUC: \", rf_model.best_score_)\n",
    "print(\"Best hyperparameters: \", rf_model.best_params_)\n",
    "\n",
    "AUC.append(rf_model.best_score_)\n",
    "Param.append(rf_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction and then calculate the accuracy of model\n",
    "y_test_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on predicted data\n",
    "confusion_score = confusion_matrix(y_test, y_test_pred,  labels=[1,0])\n",
    "print(confusion_score)\n",
    "\n",
    "# check sensitivity and specificity\n",
    "total=sum(sum(confusion_score))\n",
    "\n",
    "accuracy=(confusion_score[0,0] + confusion_score[1,1])/total\n",
    "print ('Accuracy : ', accuracy)\n",
    "\n",
    "sensitivity = confusion_score[0,0]/(confusion_score[0,0] + confusion_score[0,1])\n",
    "print('Sensitivity : ', sensitivity )\n",
    "\n",
    "specificity = confusion_score[1,1]/(confusion_score[1,0] + confusion_score[1,1])\n",
    "print('Specificity : ', specificity)\n",
    "\n",
    "ModelName.append('Ramdon Forest')\n",
    "Accuracy.append(accuracy)\n",
    "Sensitivity.append(sensitivity)\n",
    "Specificity.append(specificity)\n",
    "\n",
    "\n",
    "print('Specificity is very bad.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC\n",
    "# check area under curve\n",
    "y_pred_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))\n",
    "\n",
    "ROC.append(round(roc_auc_score(y_test, y_pred_prob),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Hyper parameter seclection for model\n",
    "- max_fearutre  = number of variables to consider to split each node. The hyperparameter that controls the split-variable randomization feature of random forests. For classification problem we use Square Root of number of features.\n",
    "\n",
    "- No of estimators: General practice is to have large number of trees in the forest to stabilize the model. We will start with 5* nuber of parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model base on the best features and evaluate it.\n",
    "# run a random forest model on train data\n",
    "max_features = int(round(np.sqrt(X_train.shape[1])))    # \n",
    "print(max_features)\n",
    "random_forest_model = RandomForestClassifier(n_estimators=max_features*5, max_features=max_features, class_weight={0:0.1, 1: 0.9}, oob_score=True, random_state=45, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "random_forest_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model evaluation\n",
    "# OOB score\n",
    "random_forest_model.oob_score_\n",
    "\n",
    "# predict churn on test data\n",
    "y_test_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred,  labels=[1,0])\n",
    "print(confusion_score)\n",
    "\n",
    "# check sensitivity and specificity\n",
    "total=sum(sum(cm))\n",
    "\n",
    "accuracy=(cm[0,0] + cm[1,1])/total  #(TF+TN)/Total Pred\n",
    "print ('Accuracy : ', accuracy)\n",
    "\n",
    "sensitivity = cm[0,0]/(cm[0,0] + cm[0,1]) \n",
    "print('Sensitivity : ', sensitivity )\n",
    "\n",
    "specificity = cm[1,1]/(cm[1,0] + cm[1,1])\n",
    "print('Specificity : ', specificity)\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = random_forest_model.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))\n",
    "\n",
    "ModelName.append('Ramdon Forest 2')\n",
    "Accuracy.append(accuracy)\n",
    "Sensitivity.append(sensitivity)\n",
    "Specificity.append(specificity)\n",
    "ROC.append(round(roc_auc_score(y_test, y_pred_prob),2))\n",
    "AUC.append(random_forest_model.oob_score_)\n",
    "Param.append('NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importnace \n",
    "\n",
    "Feature importance for the random forest model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictors\n",
    "features = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance\n",
    "importance = random_forest_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame \n",
    "var_imp_df = pd.DataFrame({'variables':features, 'imp_per':importance*100})\n",
    "var_imp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp_df = var_imp_df.sort_values('imp_per', ascending=False).reset_index(drop=True)\n",
    "var_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp_df['imp_per'][0:100].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build logistic regression model using top 60 features.\n",
    "\n",
    "Top 60 feature in the model explains the 89% of variance. We will use top 100 features to build logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_60_variables = var_imp_df.variables[0:59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] =(15,15)\n",
    "corr = sns.diverging_palette(199, 359, s=99, center=\"light\", as_cmap=True)\n",
    "sns.heatmap(data=X_train[top_60_variables].corr(), center=0.0, cmap=corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test dataset\n",
    "X_train_top_60 = X_train[top_60_variables]\n",
    "X_test_top_60 = X_test[top_60_variables]\n",
    "\n",
    "print(X_train_top_60.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Pipeline</b> \n",
    "sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.\n",
    "\n",
    "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "steps = [('scaler', StandardScaler()), \n",
    "         (\"logistic\", LogisticRegression(class_weight={0:0.1, 1:0.9}))\n",
    "        ]\n",
    "\n",
    "# initiate pipeline\n",
    "log_regression = Pipeline(steps)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {'logistic__C': [0.001, 0.01, 0.1, 0.5, 1, 2, 3, 4, 5, 10,50,100, ], 'logistic__penalty': ['l1', 'l2']}\n",
    "\n",
    "# create 5 validation sets\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 45)\n",
    "\n",
    "# create gridsearch object\n",
    "ensamble = GridSearchCV(estimator=log_regression, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "ensamble.fit(X_train_top_60, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best AUC: \", ensamble.best_score_)\n",
    "print(\"Best hyperparameters: \", ensamble.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on test data\n",
    "y_pred_test = ensamble.predict(X_test_top_60)\n",
    "\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test,  labels=[1,0])\n",
    "print(cm)\n",
    "# check sensitivity and specificity\n",
    "total=sum(sum(cm))\n",
    "\n",
    "accuracy=(cm[0,0] + cm[1,1])/total  #(TF+TN)/Total Pred\n",
    "print ('Accuracy : ', accuracy)\n",
    "\n",
    "sensitivity = cm[0,0]/(cm[0,0] + cm[0,1]) \n",
    "print('Sensitivity : ', sensitivity )\n",
    "\n",
    "specificity = cm[1,1]/(cm[1,0] + cm[1,1])\n",
    "print('Specificity : ', specificity)\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = ensamble.predict_proba(X_test_top_60)[:, 1]\n",
    "print(\"ROC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))\n",
    "\n",
    "ModelName.append('Logistic Regression with Top 60 Param from RF')\n",
    "Accuracy.append(accuracy)\n",
    "Sensitivity.append(sensitivity)\n",
    "Specificity.append(specificity)\n",
    "ROC.append(round(roc_auc_score(y_test, y_pred_prob),2))\n",
    "AUC.append(ensamble.best_score_)\n",
    "Param.append(ensamble.best_params_)\n",
    "\n",
    "print('Decent model with good accuracy and specificity. ROC score is also good.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensamble.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = ensamble.best_estimator_.named_steps['logistic']\n",
    "logistic_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_df = pd.DataFrame(logistic_model.coef_, columns=list(X_test_top_60)).T.reset_index()\n",
    "log_df.columns = ['Feature','Coefficient']\n",
    "log_df.sort_values(by='Coefficient', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data imputation but Taking too much time\n",
    "\n",
    "## using fast knn for imputation\n",
    "#from impyute.imputation.cs import fast_knn\n",
    "\n",
    "#imputed_df = fast_knn(churn_df[missing_values_columns].values, k=2)\n",
    "\n",
    "## 1st install impyute\n",
    "#from impyute.imputation.cs import mice\n",
    "\n",
    "#imputed_df = mice(churn_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check if stratify worked in splitting the target variable proportinately\n",
    "\n",
    "print(y_train.value_counts(1))\n",
    "print('\\n')\n",
    "print(y_test.value_counts(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reshaping the dependent variable df\n",
    "\n",
    "# y_train = y_train.values.reshape(-1,1)\n",
    "# y_test = y_test.values.reshape(-1,1)\n",
    "\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Scaling the data for model with good prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "# transform train data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "print(X_train_scaled[:5,:10])\n",
    "print('\\n')\n",
    "# transform test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(X_test_scaled[:5,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Trying to fit and check the variance using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising the PCA\n",
    "pca_1 = PCA(random_state=10)\n",
    "\n",
    "# fitting the PCA\n",
    "pca_1.fit(X_train_scaled)\n",
    "\n",
    "pca_1.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking the variance ratio\n",
    "pca_1.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking cumulative sum\n",
    "variance_cumsum = np.cumsum(pca_1.explained_variance_ratio_)\n",
    "\n",
    "# plotting scree plot to see the variance explained by features\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(len(variance_cumsum)), variance_cumsum)\n",
    "plt.title('Scree Plot\\n')\n",
    "plt.plot()\n",
    "\n",
    "# it seems that the 90-95% variance is explained by only 90 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at explainded variance of PCA components\n",
    "print(pd.Series(np.round(pca_1.explained_variance_ratio_.cumsum(), 4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('90% variance is explained by 57 variables.')\n",
    "print('95% variance is explained by 73 variables.')\n",
    "print('98% variance is explained by 89 variables.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Let's fit the data using Incremental PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking how many components explain the 95 percent variance using unsupervised way\n",
    "\n",
    "pca_2_unsup = PCA(0.95, random_state=10)\n",
    "\n",
    "X_train_pca2 = pca_2_unsup.fit_transform(X_train_scaled)\n",
    "\n",
    "X_train_pca2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using IncrementalPCA to fit and transform the data\n",
    "pca_inc = IncrementalPCA(X_train_pca2.shape[1])\n",
    "\n",
    "# fitting the pca on train data\n",
    "X_train_pca = pca_inc.fit_transform(X_train_scaled)\n",
    "print(X_train_pca.shape)\n",
    "\n",
    "# transforming the test data\n",
    "X_test_pca = pca_inc.transform(X_test_scaled)\n",
    "print(X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating correlation matrix\n",
    "corr_mat = np.corrcoef(X_train_pca.transpose())\n",
    "\n",
    "# we still have too many features, but we can see that in heatmap that there is no correlation now\n",
    "plt.figure(figsize=(18,10))\n",
    "sns.heatmap(corr_mat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Random Forest Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest classifier model\n",
    "rf_model = RandomForestClassifier(random_state=10, n_jobs=-1, class_weight={0:1, 1: 15})\n",
    "\n",
    "### setting parameter grid for hyper parameter tuning\n",
    "## these values below are set after trying to run the model on different values and set it to close values,\n",
    "# so that it doesn't take too much time fitting it\n",
    "\n",
    "params = {\n",
    "    'max_depth' : [30],\n",
    "    'min_samples_leaf' : [20],\n",
    "    'min_samples_split' : [20],\n",
    "    'max_features' : [35],\n",
    "    'n_estimators' : [100]\n",
    "}\n",
    "\n",
    "\n",
    "# using GridSearchCV for hyper parameter tuning\n",
    "grid_model = GridSearchCV(estimator=rf_model, param_grid=params, verbose=1, cv=4, n_jobs=-1, scoring=\"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "grid_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Print model with best parameters \n",
    "print(\"Best AUC: \", grid_model.best_score_)\n",
    "print(\"Best hyperparameters: \", grid_model.best_params_)\n",
    "\n",
    "# best model\n",
    "rf_best = grid_model.best_estimator_\n",
    "\n",
    "rf_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_best = RandomForestClassifier(random_state=10, class_weight={0:1, 1: 15},\n",
    "#                                  max_depth= 30,max_features= 35, min_samples_leaf= 20,min_samples_split= 20,\n",
    "#                                  n_estimators= 100)\n",
    "# rf_best.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make prediction on train data and then calculate the accuracy of model\n",
    "y_train_pred = rf_best.predict(X_train_pca)\n",
    "\n",
    "# Evaluate the model on predicted data\n",
    "confusion_score = confusion_matrix(y_train, y_train_pred)\n",
    "print('Train Confusion Matrix: \\n', confusion_score)\n",
    "\n",
    "# Confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, y_train_pred).ravel()\n",
    "print(f'Train Confusion metrics values: {tn, fp, fn, tp}')\n",
    "\n",
    "# Area under curve\n",
    "print(f'Train Area under the curve: {roc_auc_score(y_train, y_train_pred)}')\n",
    "\n",
    "# check sensitivity and specificity\n",
    "total=sum(sum(confusion_score))\n",
    "\n",
    "accuracy= accuracy_score(y_train, y_train_pred)\n",
    "print ('Train Accuracy : ', accuracy)\n",
    "\n",
    "sensitivity = tp/(tp+fn)\n",
    "print('Train Sensitivity : ', sensitivity )\n",
    "\n",
    "specificity = tn/(tn+fp)\n",
    "print('Train Specificity : ', specificity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on test data and then calculate the accuracy of model\n",
    "y_test_pred = rf_best.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model on predicted data\n",
    "confusion_score = confusion_matrix(y_test, y_test_pred)\n",
    "print('Test Confusion Matrix: \\n', confusion_score)\n",
    "\n",
    "# Confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "print(f'Test Confusion metrics values: {tn, fp, fn, tp}')\n",
    "\n",
    "print(f'Test Area under the curve: {roc_auc_score(y_test, y_test_pred)}')\n",
    "\n",
    "# check sensitivity and specificity\n",
    "total=sum(sum(confusion_score))\n",
    "\n",
    "accuracy= accuracy_score(y_test, y_test_pred)\n",
    "print ('Test Accuracy : ', accuracy)\n",
    "\n",
    "sensitivity = tp/(tp+fn)\n",
    "print('Test Sensitivity : ', sensitivity )\n",
    "\n",
    "specificity = tn/(tn+fp)\n",
    "print('Test Specificity : ', specificity)\n",
    "\n",
    "ModelName.append('Random Forest with PCA')\n",
    "Accuracy.append(accuracy)\n",
    "Sensitivity.append(sensitivity)\n",
    "Specificity.append(specificity)\n",
    "ROC.append(round(roc_auc_score(y_test, y_test_pred),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC.append(np.nan)\n",
    "Param.append(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Poor sensitivity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance, but these are pca derived \n",
    "rf_best.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing sample tree\n",
    "dot_data = StringIO()\n",
    "\n",
    "export_graphviz(rf_best.estimators_[0] , out_file=dot_data, filled=True, rounded=True, \n",
    "               class_names=['Not Churn', 'Churn'])\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with the PCA\n",
    "\n",
    "We will use logisti regression model based on the PCA selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_VARS = 75\n",
    "steps = [\n",
    "        ('scaler', StandardScaler()),\n",
    "        (\"pca\", PCA(n_components=PCA_VARS)),\n",
    "         (\"logistic\", LogisticRegression(class_weight='balanced'))\n",
    "        ]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# fit model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# check score on train data\n",
    "pipeline.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred,  labels=[1,0])\n",
    "print(cm)\n",
    "# check sensitivity and specificity\n",
    "total=sum(sum(cm))\n",
    "\n",
    "accuracy=(cm[0,0] + cm[1,1])/total  #(TF+TN)/Total Pred\n",
    "print ('Accuracy : ', accuracy)\n",
    "\n",
    "sensitivity = cm[0,0]/(cm[0,0] + cm[0,1]) \n",
    "print('Sensitivity : ', sensitivity )\n",
    "\n",
    "specificity = cm[1,1]/(cm[1,0] + cm[1,1])\n",
    "print('Specificity : ', specificity)\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))\n",
    "\n",
    "ModelName.append('Logistic Regression with 75 PCA')\n",
    "Accuracy.append(accuracy)\n",
    "Sensitivity.append(sensitivity)\n",
    "Specificity.append(specificity)\n",
    "ROC.append(round(roc_auc_score(y_test, y_test_pred),2))\n",
    "AUC.append(np.nan)\n",
    "Param.append(np.nan)\n",
    "\n",
    "print('Decent model with good accuracy and specificity. ROC score is also good.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for the PCA with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA()\n",
    "\n",
    "logistic = LogisticRegression(class_weight={0:0.1, 1: 0.9})\n",
    "\n",
    "# create pipeline\n",
    "steps = [(\"scaler\", StandardScaler()), \n",
    "         (\"pca\", pca),\n",
    "         (\"logistic\", logistic)\n",
    "        ]\n",
    "\n",
    "# compile pipeline\n",
    "pca_logistic = Pipeline(steps)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {'pca__n_components': [55, 75, 85], 'logistic__C': [0.1, 0.5, 1, 2, 3, 4, 5, 10], 'logistic__penalty': ['l1', 'l2']}\n",
    "\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 45)\n",
    "\n",
    "# create gridsearch object\n",
    "model = GridSearchCV(estimator=pca_logistic, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation results\n",
    "pd.DataFrame(model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best AUC: \", model.best_score_)\n",
    "print(\"Best hyperparameters: \", model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred,  labels=[1,0])\n",
    "print(cm)\n",
    "# check sensitivity and specificity\n",
    "total=sum(sum(cm))\n",
    "\n",
    "accuracy=(cm[0,0] + cm[1,1])/total  #(TF+TN)/Total Pred\n",
    "print ('Accuracy : ', accuracy)\n",
    "\n",
    "sensitivity = cm[0,0]/(cm[0,0] + cm[0,1]) \n",
    "print('Sensitivity : ', sensitivity )\n",
    "\n",
    "specificity = cm[1,1]/(cm[1,0] + cm[1,1])\n",
    "print('Specificity : ', specificity)\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))\n",
    "\n",
    "ModelName.append('Logistic Regression with 85 PCA')\n",
    "Accuracy.append(accuracy)\n",
    "Sensitivity.append(sensitivity)\n",
    "Specificity.append(specificity)\n",
    "ROC.append(round(roc_auc_score(y_test, y_pred_prob),2))\n",
    "AUC.append(model.best_score_)\n",
    "Param.append(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We built 5 models in this excercise. </b>\n",
    "- 1. Random Forest for prediction as well as for feature importance\n",
    "- 2. Logistic Regression based on the important features from the random forest ensamble.\n",
    "- 3. PCA for feature selection.\n",
    "- 4. Random forest with the PCA for prediction.\n",
    "- 5. Logistic Regression with PCA for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'Model Name':ModelName, 'Accuracy':Accuracy,'Sensitivity':Sensitivity,'Specificity':Specificity,'ROC':ROC,'AUC':AUC,'Parameter':Param})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Logistic regresion works well for the identification of the churn with good accuracy, sensitivity and specificity score.\n",
    "     this will be the the algorithm that would be used for the prediction.\n",
    "-  PCA is simple an dquick way to identify the important parameters. However important parameter extracted from the ensamble aslo gives good explanation of feature selection with weitage factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recommend strategies to manage customer churn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Company should offer better roaming packages to the customer.\n",
    "2. total incoming and outgoing calls plays have higher influance on the customer retaintion. A better calling esperiance and package would make impact on the churn rate.\n",
    "3. Titmly reddresal of grivenace by the company would benifit in retaining the customers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
